{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salixkang/COSE474_Final/blob/main/Final_Project_2018320223_%EA%B0%95%EC%8A%B9%EB%AA%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keDMok7Ur2sr"
      },
      "source": [
        "Final Project 2018320223 컴퓨터학과 강승모"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un3BqHiReT3W",
        "outputId": "c9fc6a8f-73cc-471f-8499-e7030c3de900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/DL_Final Project\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import csv\n",
        "\n",
        "from itertools import count\n",
        "from collections import namedtuple, deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from google.colab import drive\n",
        "root = '/content/drive/'\n",
        "drive.mount(root)\n",
        "%cd '/content/drive/MyDrive/DL_Final Project'\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "# if GPU is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzH5aFU_q70w"
      },
      "source": [
        "#TileGameEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v5__vczsqQkE"
      },
      "outputs": [],
      "source": [
        "class TileGameEnv:\n",
        "    def __init__(self, layout, max_card_usage, max_swap):\n",
        "        self.initial_layout = np.array(layout)\n",
        "        self.layout = np.copy(self.initial_layout)\n",
        "        self.max_card_usage = max_card_usage\n",
        "        self.total_normal_card_types = 10\n",
        "        self.max_enhancement = 2\n",
        "        self.max_swap = max_swap\n",
        "        self.action_types = [0, 1, 2, 3]  # 0: Use Card 1, 1: Use Card 2, 2: Swap Card 1, 3: Swap Card 2\n",
        "        self.action_space = []\n",
        "        self._get_action_space()\n",
        "        self.tile_types = ['empty', '정상', '왜곡', '재배치', '축복', '추가', '강화', '복제', '신비']\n",
        "        self.special_tile_effects = {\n",
        "            '재배치': self._effect_rearrange,\n",
        "            '축복': self._effect_blessing,\n",
        "            '추가': self._effect_extra,\n",
        "            '강화': self._effect_enhance,\n",
        "            '복제': self._effect_clone,\n",
        "            '신비': self._effect_mystery\n",
        "        }\n",
        "        self.card_types = ['충격파', '낙뢰', '용오름', '해일', '지진', '폭풍우', '대폭발', '정화', '벼락', '업화', '분출', '세계수의 공명']\n",
        "        self.card_effects = {\n",
        "            '충격파': self._effect_shockwave,\n",
        "            '낙뢰': self._effect_thunder,\n",
        "            '용오름': self._effect_dragon_rise,\n",
        "            '해일': self._effect_tidal_wave,\n",
        "            '지진': self._effect_earthquake,\n",
        "            '폭풍우': self._effect_storm,\n",
        "            '대폭발': self._effect_explosion,\n",
        "            '정화': self._effect_purification,\n",
        "            '벼락': self._effect_lightning,\n",
        "            '업화': self._effect_upfire,\n",
        "            '분출': self._effect_eruption,\n",
        "            '세계수의 공명': self._effect_world_tree_resonance\n",
        "        }\n",
        "        self.current_special_tile = None  # 현재 스텝에서 생성된 특수 타일의 위치\n",
        "        self.hand = []\n",
        "        self.reserve = []\n",
        "        self.hand_enhancements = [0, 0]\n",
        "        self.cards_used = 0\n",
        "        self.swap_count = 0\n",
        "        self.used_card_index = None\n",
        "        self.reset()\n",
        "\n",
        "    def _get_action_space(self):\n",
        "        # action space 정의\n",
        "        for x in range(self.initial_layout.shape[0]):\n",
        "            for y in range(self.initial_layout.shape[1]):\n",
        "                for type in [0, 1]:\n",
        "                    self.action_space.append([type, x, y])\n",
        "\n",
        "        for type in [2, 3]:\n",
        "            self.action_space.append([type, -1, -1])\n",
        "\n",
        "    def reset(self):\n",
        "        # 게임 초기화\n",
        "        self.layout = np.copy(self.initial_layout)\n",
        "        self.hand = [random.randint(0, self.total_normal_card_types - 1) for _ in range(2)]  # 두 장의 패\n",
        "        self.reserve = [random.randint(0, self.total_normal_card_types - 1) for _ in range(3)]  # 세 장의 다음 카드\n",
        "        self.hand_enhancements = [0, 0]  # 패에 있는 각 카드의 강화 상태\n",
        "        self.cards_used = 0\n",
        "        self.swap_count = 0\n",
        "        self.used_card_index = None\n",
        "        self.current_special_tile = None\n",
        "        self._check_enhancement(0)\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self):\n",
        "        return np.array(self.layout.flatten().tolist() + self.hand + self.reserve + self.hand_enhancements + [self.cards_used] + [self.swap_count])\n",
        "\n",
        "    def step(self, action):\n",
        "        action_type, x, y = self.action_space[action]\n",
        "        position = (x, y)\n",
        "        reward = 0\n",
        "\n",
        "        before_count_normal_tiles = np.sum((self.layout == 1) | ((3 <= self.layout) & (self.layout < 9)))\n",
        "\n",
        "        # Action: Use Card 1\n",
        "        if action_type == 0:\n",
        "            self.used_card_index = 0\n",
        "            self._play_card(0, position)\n",
        "            if self.current_special_tile:\n",
        "                self.layout[self.current_special_tile] = 1\n",
        "                self._generate_special_tile()\n",
        "            else:\n",
        "                self._generate_special_tile()\n",
        "\n",
        "        # Action: Use Card 2\n",
        "        elif action_type == 1:\n",
        "            self.used_card_index = 1\n",
        "            self._play_card(1, position)\n",
        "            if self.current_special_tile:\n",
        "                self.layout[self.current_special_tile] = 1\n",
        "                self._generate_special_tile()\n",
        "            else:\n",
        "                self._generate_special_tile()\n",
        "\n",
        "        # Action: Swap Card 1\n",
        "        elif action_type == 2:\n",
        "            self._swap_card(0)\n",
        "\n",
        "        # Action: Swap Card 2\n",
        "        elif action_type == 3:\n",
        "            self._swap_card(1)\n",
        "\n",
        "        after_count_normal_tiles = np.sum((self.layout == 1) | ((3 <= self.layout) & (self.layout < 9)))\n",
        "\n",
        "        done, result = self._is_game_over()\n",
        "        if result == \"Win\":\n",
        "            reward = 200\n",
        "        else:\n",
        "            reward += before_count_normal_tiles - after_count_normal_tiles\n",
        "\n",
        "        return self._get_state(), reward, done, {}\n",
        "\n",
        "    # 선택된 카드를 사용하는 메소드\n",
        "    def _play_card(self, card_index, position):\n",
        "        card_type = self.card_types[self.hand[card_index]]\n",
        "\n",
        "        card_effect = self.card_effects.get(card_type, None)\n",
        "\n",
        "        if card_effect:\n",
        "            card_effect(position, self.hand_enhancements[card_index])\n",
        "\n",
        "        self.hand[card_index] = self.reserve.pop(0)  # 예비 카드로 교체\n",
        "        self.hand_enhancements[card_index] = 0  # 사용된 카드 강화 상태 리셋\n",
        "\n",
        "        self.reserve.append(self._draw_new_card())  # 예비 카드 보충\n",
        "        self.cards_used += 1  # 카드 사용 횟수 증가\n",
        "\n",
        "        # 카드가 강화 가능한지 체크\n",
        "        self._check_enhancement(card_index)\n",
        "\n",
        "\n",
        "    # 카드를 교체하는 메소드\n",
        "    def _swap_card(self, card_index):\n",
        "        if self.swap_count < self.max_swap:\n",
        "            self.swap_count += 1  # 교체 횟수 증가\n",
        "            self.hand[card_index] = self.reserve.pop(0)  # 예비 카드로 교체\n",
        "            self.hand_enhancements[card_index] = 0  # 사용된 카드 강화 상태 리셋\n",
        "            self.reserve.append(self._draw_new_card())  # 예비 카드 보충\n",
        "            self._check_enhancement(card_index)  # 강화 로직 적용\n",
        "\n",
        "    # 새로운 카드를 예비 카드에 추가하는 메소드\n",
        "    def _draw_new_card(self):\n",
        "        return random.randint(0, self.total_normal_card_types - 1)\n",
        "\n",
        "    def _check_enhancement(self, new_card_index):\n",
        "        other_card_index = 1 - new_card_index\n",
        "        if self.hand[new_card_index] == self.hand[other_card_index]:\n",
        "            if self.hand_enhancements[other_card_index] < self.max_enhancement:\n",
        "                self.hand_enhancements[other_card_index] += 1  # 카드 강화\n",
        "                self.hand[new_card_index] = self.reserve.pop(0)  # 다음 예비 카드를 패로 이동\n",
        "                self.hand_enhancements[new_card_index] = 0\n",
        "                self.reserve.append(self._draw_new_card())  # 예비 카드 보충\n",
        "                self._check_enhancement(new_card_index)\n",
        "\n",
        "    # 타일 파괴\n",
        "    def _destroy_tile(self, position, enhancement=0, special=0):\n",
        "        x, y = position\n",
        "        tile_index = self.layout[x, y]\n",
        "\n",
        "        if tile_index == 1:\n",
        "            self.layout[x, y] = 0\n",
        "        elif tile_index == 2:\n",
        "            self._destroy_distorted_tile(position, enhancement, special)\n",
        "        elif tile_index in range(3, 9):\n",
        "            tile_type = self.tile_types[tile_index]\n",
        "            special_tile_effect = self.special_tile_effects.get(tile_type, None)\n",
        "            self.layout[x, y] = 0\n",
        "            self.current_special_tile = None\n",
        "            if special_tile_effect:\n",
        "                special_tile_effect()\n",
        "\n",
        "    # 특정 위치가 게임판 내에 있는지 확인하는 메소드\n",
        "    def _is_within_bounds(self, position):\n",
        "        x, y = position\n",
        "        return 0 <= x < self.layout.shape[0] and 0 <= y < self.layout.shape[1]\n",
        "\n",
        "    # 랜덤 위치에 정상 타일을 생성하는 메소드\n",
        "    def _create_random_tiles(self, count):\n",
        "        for _ in range(count):\n",
        "            empty_positions = [(i, j) for i in range(self.layout.shape[0])\n",
        "                               for j in range(self.layout.shape[1]) if self.layout[i][j] == 0]\n",
        "            if empty_positions:\n",
        "                x, y = random.choice(empty_positions)\n",
        "                self.layout[x, y] = 1  # 정상 타일 생성\n",
        "\n",
        "    # 왜곡된 타일을 파괴했을 때 정상 타일을 생성하는 메서드\n",
        "    def _destroy_distorted_tile(self, position, enhancement, special=0):\n",
        "        # 파괴할 위치에 왜곡된 타일이 있는지 확인\n",
        "        if self.layout[position] == 2:\n",
        "            self.layout[position] = 0  # 왜곡된 타일 파괴\n",
        "\n",
        "            if enhancement != 2 and special != 1:  # 강화 단계가 2거나 특수한 상황이라면 페널티 발생하지 않음\n",
        "                # 타일이 없는 위치 중 무작위로 세 개의 위치에 정상 타일 생성\n",
        "                empty_positions = np.argwhere(self.layout == 0)\n",
        "                if empty_positions.size > 0:\n",
        "                    for _ in range(min(3, len(empty_positions))):  # 최대 3개 혹은 가능한 개수만큼\n",
        "                        x, y = empty_positions[random.randint(0, len(empty_positions) - 1)]\n",
        "                        self.layout[x, y] = 1  # 정상 타일 생성\n",
        "\n",
        "    # 남은 정상 타일 중 하나에 무작위 특수 타일 생성 메소드\n",
        "    def _generate_special_tile(self):\n",
        "        normal_tiles = list(zip(*np.where(self.layout == 1)))\n",
        "        if normal_tiles:  # 정상 타일이 존재하는 경우\n",
        "            x, y = random.choice(normal_tiles)\n",
        "            special_tile_type = random.randint(3, len(self.tile_types) - 1)\n",
        "            self.layout[x, y] = special_tile_type\n",
        "            self.current_special_tile = (x, y)  # 생성된 특수 타일의 위치 저장\n",
        "\n",
        "    # 충격파\n",
        "    def _effect_shockwave(self, position, enhancement):\n",
        "        # 중심 위치의 타일은 100% 확률로 파괴\n",
        "        center_x, center_y = position\n",
        "        self._destroy_tile((center_x, center_y), enhancement)\n",
        "        # 3x3 영역의 타일을 파괴하는 로직\n",
        "        for i in range(center_x - 1, center_x + 2):\n",
        "            for j in range(center_y - 1, center_y + 2):\n",
        "                # 배열의 범위를 벗어나지 않는지 체크\n",
        "                if self._is_within_bounds((i, j)):\n",
        "                    # 중심 타일이 아닌 경우에만 확률 체크\n",
        "                    if (i, j) != position:\n",
        "                        if random.random() < 0.75 or enhancement > 0:  # 75% or 강화되면 파괴\n",
        "                            self._destroy_tile((i, j))\n",
        "    # 낙뢰\n",
        "    def _effect_thunder(self, position, enhancement):\n",
        "        # 중심 위치는 항상 파괴\n",
        "        center_x, center_y = position\n",
        "        self._destroy_tile((center_x, center_y), enhancement)\n",
        "        # 십자가 형태로 영향을 주므로 상하좌우 타일 파괴\n",
        "        offsets = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "        for dx, dy in offsets:\n",
        "            x, y = center_x + dx, center_y + dy\n",
        "            if self._is_within_bounds((x, y)):\n",
        "                if random.random() < 0.5 or enhancement > 0:  # 50% 확률 or 강화되면 파괴\n",
        "                    self._destroy_tile((x, y), enhancement)\n",
        "    # 용오름\n",
        "    def _effect_dragon_rise(self, position, enhancement):\n",
        "        # 중심 위치는 항상 파괴\n",
        "        center_x, center_y = position\n",
        "        self._destroy_tile((center_x, center_y), enhancement)\n",
        "        # X 자 형태로 영향을 주므로 대각선 타일 파괴\n",
        "        diagonals = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "        for dx, dy in diagonals:\n",
        "            x, y = center_x + dx, center_y + dy\n",
        "            if self._is_within_bounds((x, y)):\n",
        "                if random.random() < 0.5 or enhancement > 0:  # 50% 확률로 or 강화되면 파괴\n",
        "                    self._destroy_tile((x, y), enhancement)\n",
        "\n",
        "    # 해일\n",
        "    def _effect_tidal_wave(self, position, enhancement):\n",
        "        # 중심 위치는 100% 확률로 파괴\n",
        "        center_x, center_y = position\n",
        "        self._destroy_tile((center_x, center_y), enhancement)\n",
        "        # 파괴 확률을 위한 기본값 설정\n",
        "        base_prob = 1.0\n",
        "        decrease_step = 0.15\n",
        "\n",
        "        # 십자가 형태로 타일을 파괴\n",
        "        # 상하 방향\n",
        "        for i in range(self.layout.shape[0]):\n",
        "            if i != center_x:  # 중심 위치 제외\n",
        "                if self._is_within_bounds((i, center_y)):\n",
        "                    destroy_prob = base_prob - decrease_step * abs(i - center_x)\n",
        "                    if random.random() < destroy_prob or enhancement > 0:\n",
        "                        self._destroy_tile((i, center_y), enhancement)\n",
        "\n",
        "        # 좌우 방향\n",
        "        for j in range(self.layout.shape[1]):\n",
        "            if j != center_y:  # 중심 위치 제외\n",
        "                if self._is_within_bounds((center_x, j)):\n",
        "                    destroy_prob = base_prob - decrease_step * abs(j - center_y)\n",
        "                    if random.random() < destroy_prob or enhancement > 0:\n",
        "                        self._destroy_tile((center_x, j), enhancement)\n",
        "\n",
        "\n",
        "    # 지진\n",
        "    def _effect_earthquake(self, position, enhancement):\n",
        "        # '지진' 카드: 시전 위치를 중심으로 가로 일렬 범위 모든 타일 타격\n",
        "        for j in range(self.layout.shape[1]):\n",
        "            if self._is_within_bounds((position[0], j)):\n",
        "                destroy_prob = 1.0 - 0.15 * abs(j - position[1])\n",
        "                if random.random() < destroy_prob or enhancement > 0:\n",
        "                    self._destroy_tile((position[0], j), enhancement)\n",
        "\n",
        "\n",
        "    # 폭풍우\n",
        "    def _effect_storm(self, position, enhancement):\n",
        "        # '폭풍우' 카드: 시전 위치를 중심으로 세로 일렬 범위 모든 타일 타격\n",
        "\n",
        "        for i in range(self.layout.shape[0]):\n",
        "            if self._is_within_bounds((i, position[1])):\n",
        "                destroy_prob = 1.0 - 0.15 * abs(i - position[0])\n",
        "                if random.random() < destroy_prob or enhancement > 0:\n",
        "                    self._destroy_tile((i, position[1]), enhancement)\n",
        "\n",
        "    # 대폭발\n",
        "    def _effect_explosion(self, position, enhancement):\n",
        "        # '대폭발' 카드: 시전 위치 중심으로 X 자 형태 모든 타일 타격\n",
        "\n",
        "        for offset in range(-self.layout.shape[0], self.layout.shape[0]):\n",
        "            destroy_prob = 1.0 - 0.15 * abs(offset)\n",
        "            if random.random() < destroy_prob or enhancement > 0:\n",
        "                if self._is_within_bounds((position[0] + offset, position[1] + offset)):\n",
        "                    self._destroy_tile((position[0] + offset, position[1] + offset), enhancement)\n",
        "                if self._is_within_bounds((position[0] + offset, position[1] - offset)):\n",
        "                    self._destroy_tile((position[0] + offset, position[1] - offset), enhancement)\n",
        "\n",
        "\n",
        "    # 정화\n",
        "    def _effect_purification(self, position, enhancement):\n",
        "        # '정화' 카드: 시전 위치 중심으로 좌우 타일 타격\n",
        "        special = 1\n",
        "        center_x, center_y = position\n",
        "        self._destroy_tile((center_x, center_y), enhancement, special)  # 중심 타일은 항상 파괴\n",
        "        for offset in [-1, 1]:\n",
        "            if 0 <= position[1] + offset < self.layout.shape[1]:\n",
        "                if random.random() < 0.5 or enhancement > 0:  # 좌우 타일은 50% 확률로 or 강화되면 파괴\n",
        "                    self._destroy_tile((position[0], position[1] + offset), enhancement, special)\n",
        "            if enhancement == 2:\n",
        "                if 0 <= position[0] + offset < self.layout.shape[0]:\n",
        "                    self._destroy_tile((position[0] + offset, position[1]), enhancement, special)\n",
        "\n",
        "\n",
        "    # 벼락\n",
        "    def _effect_lightning(self, position, enhancement):\n",
        "        # '벼락' 카드: 시전 위치 100% 파괴, 랜덤으로 0~2칸 추가 파괴\n",
        "        self._destroy_tile(position)\n",
        "\n",
        "        all_tiles = [(i, j) for i in range(self.layout.shape[0])\n",
        "                     for j in range(self.layout.shape[1])\n",
        "                     if self.layout[i, j] != 2 and self.layout[i, j] != 0 and self.layout[i, j] != -1]\n",
        "\n",
        "        num_tiles_to_destroy = random.randint(0, min(2 + 2 * enhancement, len(all_tiles)))\n",
        "\n",
        "        for _ in range(num_tiles_to_destroy):\n",
        "            tile_to_destroy = random.choice(all_tiles)\n",
        "            self._destroy_tile(tile_to_destroy)\n",
        "            all_tiles.remove(tile_to_destroy)  # 파괴된 타일은 리스트에서 제거\n",
        "\n",
        "        # 랜덤 위치에 랜덤으로 0~1칸 정상 타일 생성\n",
        "        self._create_random_tiles(random.randint(0, 1))\n",
        "\n",
        "    # 업화\n",
        "    def _effect_upfire(self, position, enhancement):\n",
        "        # '업화' 카드: 시전 위치 중심으로 마름모 형태 2칸 범위 내 파괴, 시전 위치 100% 파괴\n",
        "        self._destroy_tile(position, enhancement)\n",
        "        offsets = [(-2, 0), (-1, -1), (-1, 0), (-1, 1), (0, -2), (0, -1), (0, 1), (0, 2), (1, -1), (1, 0), (1, 1),\n",
        "                   (2, 0)]\n",
        "        for dx, dy in offsets:\n",
        "            new_position = (position[0] + dx, position[1] + dy)\n",
        "            if self._is_within_bounds(new_position) and random.random() < 0.5:\n",
        "                if random.random() < 0.5 or enhancement > 0:\n",
        "                    self._destroy_tile(new_position, enhancement)\n",
        "\n",
        "\n",
        "    # 분출\n",
        "    def _effect_eruption(self, position, enhancement=0):\n",
        "        # '분출' 카드: 시전 위치 100% 파괴\n",
        "        self._destroy_tile(position)\n",
        "\n",
        "    # 세계수의 공명\n",
        "    def _effect_world_tree_resonance(self, position, enhancement=0):\n",
        "        # '세계수의 공명' 카드: 시전 위치 중심으로 십자형 타일 2칸 범위 100% 파괴\n",
        "        special = 1\n",
        "        offsets = [(0, 0), (-2, 0), (-1, 0), (1, 0), (2, 0), (0, -2), (0, -1), (0, 1), (0, 2)]\n",
        "        for dx, dy in offsets:\n",
        "            new_position = (position[0] + dx, position[1] + dy)\n",
        "            if self._is_within_bounds(new_position):\n",
        "                self._destroy_tile(new_position, enhancement, special)\n",
        "\n",
        "\n",
        "    # 특수 타일의 효과를 적용하는 메소드들\n",
        "    def _effect_rearrange(self):\n",
        "        # '재배치' 효과: 남은 모든 타일을 무작위 위치로 재배치\n",
        "        tiles = [(x, y, self.layout[x, y]) for x in range(self.layout.shape[0])\n",
        "                 for y in range(self.layout.shape[1]) if self.layout[x, y] != -1]\n",
        "\n",
        "        # 비정상 위치를 제외한 나머지 타일 값만 추출\n",
        "        tile_values = [value for _, _, value in tiles if value != -1]\n",
        "\n",
        "        # 추출한 타일 값을 무작위로 섞음\n",
        "        random.shuffle(tile_values)\n",
        "\n",
        "        # 무작위로 섞은 타일 값을 다시 게임 보드에 배치\n",
        "        for (x, y, _), value in zip(tiles, tile_values):\n",
        "            self.layout[x, y] = value\n",
        "\n",
        "    def _effect_blessing(self):\n",
        "        # '축복' 효과: 이번 턴의 카드 사용 횟수를 증가시키지 않음\n",
        "        self.cards_used -= 1\n",
        "\n",
        "    def _effect_extra(self):\n",
        "        # '추가' 효과: 최대 카드 교체 횟수를 하나 늘림\n",
        "        self.max_swap += 1\n",
        "\n",
        "    def _effect_enhance(self):\n",
        "        # '강화' 효과: 패에 있는 카드를 1회 강화\n",
        "        if self.hand_enhancements[1 - self.used_card_index] < 2:\n",
        "            self.hand_enhancements[1 - self.used_card_index] += 1\n",
        "\n",
        "    def _effect_clone(self):\n",
        "        # '복제' 효과: 마지막으로 사용한 카드로 교체\n",
        "        self.hand[1 - self.used_card_index] = self.hand[self.used_card_index]\n",
        "        self.hand_enhancements[1 - self.used_card_index] = self.hand_enhancements[self.used_card_index]\n",
        "\n",
        "    def _effect_mystery(self):\n",
        "        # '신비' 효과: 랜덤하게 '세계수의 공명' 또는 '분출'로 교체\n",
        "        self.hand_enhancements[1 - self.used_card_index] = 0\n",
        "        if random.random() < 0.5:\n",
        "            self.hand[1 - self.used_card_index] = 10\n",
        "        else:\n",
        "            self.hand[1 - self.used_card_index] = 11\n",
        "\n",
        "    def _is_game_over(self):\n",
        "        # 정상 타일(1)과 특수 타일(3~9)이 없는지 확인\n",
        "        normal_tiles_gone = np.all((self.layout != 1) & (self.layout < 3))\n",
        "        # 카드 사용 및 교체 횟수가 규정 내인지 확인\n",
        "        within_card_usage = self.cards_used <= self.max_card_usage\n",
        "        within_swap_count = self.swap_count <= self.max_swap\n",
        "\n",
        "        if normal_tiles_gone and within_card_usage and within_swap_count:\n",
        "            return True, \"Win\"\n",
        "        elif not within_card_usage or not within_swap_count:\n",
        "            return True, \"Lose\"\n",
        "        else:\n",
        "            return False, \"\"\n",
        "\n",
        "    def render(self):\n",
        "        # 환경 상태 출력\n",
        "        print(\"Layout:\")\n",
        "        print(self.layout)\n",
        "        print(\"Hand: \", [self.card_types[self.hand[0]], self.card_types[self.hand[1]]])\n",
        "        print(\"Reserve: \",\n",
        "              [self.card_types[self.reserve[0]], self.card_types[self.reserve[1]], self.card_types[self.reserve[2]]])\n",
        "        print(\"Cards used: \", self.cards_used)\n",
        "        print(\"Swaps used: \", self.swap_count)\n",
        "        print(\"Enhancements: \", self.hand_enhancements)\n",
        "        print(\"--------------------------------------------------------------------------\")\n",
        "\n",
        "level0 = [\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    [1, 1, 1, 1, 1, 1, 1, 1]]\n",
        "level4 = [[-1, -1, 1, 1, 1, 1, -1, -1], [-1, 1, 1, 1, 1, 1, 1, -1], [-1, 1, 1, 1, 1, 1, 1, -1], [-1, 1, 1, 1, 1, 1, 1, -1], [-1, 1, 1, 1, 1, 1, 1, -1],\n",
        "          [-1, -1, 1, 1, 1, 1, -1, -1]]\n",
        "level5 = [[-1, -1, 1, 1, -1, -1], [-1, 1, 1, 1, 1, -1], [1, 1, 2, 2, 1, 1], [1, 1, 2, 2, 1, 1], [-1, 1, 1, 1, 1, -1],\n",
        "          [-1, -1, 1, 1, -1, -1]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWhjdHN2rDZi"
      },
      "source": [
        "#DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mV1AYuzWInIH"
      },
      "outputs": [],
      "source": [
        "# 신경망 아키텍처 정의\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_observations, 512)\n",
        "        self.layer2 = nn.Linear(512, 512)\n",
        "        self.layer3 = nn.Linear(512, 512)\n",
        "        self.layer4 = nn.Linear(512, 512)\n",
        "        self.layer5 = nn.Linear(512, 512)\n",
        "        self.layer6 = nn.Linear(512, n_actions)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = F.relu(self.layer3(x))\n",
        "        x = F.relu(self.layer4(x))\n",
        "        x = F.relu(self.layer5(x))\n",
        "        return self.layer6(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ta-QEHwyScsa"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'reward', 'next_state'))\n",
        "\n",
        "# 경험 재생 메모리 정의\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xWaz6A4lmM7U"
      },
      "outputs": [],
      "source": [
        "# DQN 에이전트 정의\n",
        "class DQNAgent:\n",
        "    def __init__(self, n_observations, n_actions, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = 0.99  # Discount factor\n",
        "        self.EPS_START = 0.9  # Exploration rate\n",
        "        self.EPS_END = 0.05\n",
        "        self.EPS_DECAY = 1000\n",
        "        self.n_observations = n_observations\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = 0.001\n",
        "        self.policy_net = DQN(self.n_observations, self.n_actions).to(device)\n",
        "        self.target_net = DQN(self.n_observations, self.n_actions).to(device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.lr, amsgrad=True)\n",
        "        self.memory = ReplayMemory(10000)\n",
        "        self.steps_done = 0\n",
        "\n",
        "    def is_action_valid(self, state, action):\n",
        "        np_state = state.cpu().numpy().reshape(-1)\n",
        "        len_layout = len(np_state[0:-9])\n",
        "        layout = np_state[0:-9].reshape(int(math.sqrt(len_layout)), int(math.sqrt(len_layout)))\n",
        "        type, x, y = action\n",
        "        if type in [0, 1]:\n",
        "            if layout[x, y] == 0 or layout[x, y] == -1:\n",
        "                return False\n",
        "            else:\n",
        "                return True\n",
        "        elif type in [2, 3]:\n",
        "            if np_state[-1] == env.max_swap:\n",
        "                return False\n",
        "            else:\n",
        "                return True\n",
        "\n",
        "    def select_action(self, state, eval):\n",
        "        sample = random.random()\n",
        "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * \\\n",
        "            math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
        "        self.steps_done += 1\n",
        "\n",
        "        if sample <= eps_threshold and eval == 0:\n",
        "            # Explore: 무작위 행동을 취함\n",
        "            valid_actions = [index for index, action in enumerate(env.action_space) if self.is_action_valid(state, action)]\n",
        "            if not valid_actions:\n",
        "                raise ValueError(\"No valid actions available\")\n",
        "            # 무작위로 선택된 유효한 액션의 인덱스를 선택\n",
        "            action_index = random.choice(valid_actions)\n",
        "            return torch.tensor([[action_index]], device=device, dtype=torch.long)\n",
        "        else:\n",
        "            # Exploit: 신경망을 사용하여 행동을 선택함\n",
        "            with torch.no_grad():\n",
        "                action_values = self.policy_net(state)\n",
        "                # 각 액션의 유효성을 평가하여 액션 마스크를 생성\n",
        "                action_mask = torch.tensor([self.is_action_valid(state, action) for action in env.action_space], device=device, dtype=torch.bool)\n",
        "                masked_action_values = action_values.masked_fill(~action_mask, float('-inf'))\n",
        "                # t.max(1) will return the largest column value of each row.\n",
        "                # second column on max result is index of where max element was\n",
        "                # found, so we pick action with the larger expected reward.\n",
        "                return masked_action_values.max(1)[1].view(1, 1)\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        transitions = self.memory.sample(self.batch_size)\n",
        "\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                    if s is not None])\n",
        "\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "        # columns of actions taken. These are the actions which would've been taken\n",
        "        # for each batch state according to policy_net\n",
        "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "        # Compute V(s_{t+1}) for all next states.\n",
        "        # Expected values of actions for non_final_next_states are computed based\n",
        "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "        # This is merged based on the mask, such that we'll have either the expected\n",
        "        # state value or 0 in case the state was final.\n",
        "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
        "        with torch.no_grad():\n",
        "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
        "        # Compute the expected Q values\n",
        "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
        "\n",
        "        # Compute Huber loss\n",
        "        criterion = nn.SmoothL1Loss()\n",
        "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # In-place gradient clipping\n",
        "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "L5BfGeNfdGjn"
      },
      "outputs": [],
      "source": [
        "train_rewards = []\n",
        "eval_rewards = []\n",
        "\n",
        "def plot_rewards(show_result=False):\n",
        "    plt.figure(1)\n",
        "    rewards_t = torch.tensor(train_rewards, dtype=torch.float)\n",
        "\n",
        "    if show_result:\n",
        "        plt.title('Game Results')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training Progress')\n",
        "\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.plot(rewards_t.numpy())\n",
        "\n",
        "    # Take 10 episode averages and plot them too\n",
        "    if len(rewards_t) >= 10:\n",
        "        means = rewards_t.unfold(0, 10, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(9), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    #plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_csv(data, filename):\n",
        "    # 파일을 추가 모드('a')로 열기\n",
        "    with open(filename, 'a', newline='') as csvfile:\n",
        "        # CSV 라이터 생성\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "\n",
        "        # 데이터를 CSV 파일에 추가\n",
        "        csv_writer.writerow(data)\n"
      ],
      "metadata": {
        "id": "WN5alBIjA0nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "w66D1Ph4pew0"
      },
      "outputs": [],
      "source": [
        "# 학습 루프 정의\n",
        "def train_dqn(env, agent, num_episodes):\n",
        "    TAU = 0.005\n",
        "    if torch.cuda.is_available():\n",
        "        num_episodes = num_episodes\n",
        "    else:\n",
        "        num_episodes = 5\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            action = agent.select_action(state, 0)\n",
        "            observation, reward, done, _ = env.step(action.item())\n",
        "            total_reward = min(200, total_reward + reward)\n",
        "            reward = torch.tensor([reward], device=device)\n",
        "            if done:\n",
        "                next_state = None\n",
        "            else:\n",
        "                next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            # Store the transition in memory\n",
        "            agent.memory.push(state, action, reward, next_state)\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "\n",
        "            # Perform one step of the optimization (on the policy network)\n",
        "            agent.train()\n",
        "\n",
        "            # Soft update of the target network's weights\n",
        "            # θ′ ← τ θ + (1 −τ )θ′\n",
        "            target_net_state_dict = agent.target_net.state_dict()\n",
        "            policy_net_state_dict = agent.policy_net.state_dict()\n",
        "            for key in policy_net_state_dict:\n",
        "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "            agent.target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "            if done:\n",
        "                train_rewards.append(total_reward)\n",
        "                torch.save({\n",
        "                    'policy_model_state_dict': agent.policy_net.state_dict(),\n",
        "                    'target_model_state_dict': agent.target_net.state_dict(),\n",
        "                    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
        "                    # 추가적인 학습 관련 상태\n",
        "                }, 'model_checkpoint.pth')\n",
        "                break\n",
        "    print('Complete')\n",
        "    plot_rewards(show_result=True)\n",
        "    plt.ioff()\n",
        "    # 저장할 디렉토리 설정\n",
        "    save_dir = './train'\n",
        "\n",
        "    # 디렉토리가 없으면 생성\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    # 이미지 파일에 붙일 인덱스 초기화\n",
        "    index = 1\n",
        "\n",
        "    # 이미지 파일이 존재하는지 확인하고, 다음 사용 가능한 인덱스 찾기\n",
        "    while os.path.exists(os.path.join(save_dir, f'train_figure_{index}.png')):\n",
        "        index += 1\n",
        "\n",
        "    # 이미지 파일로 저장 (이미지 형식은 확장자에 따라 결정됨)\n",
        "    plt.savefig(os.path.join(save_dir, f'train_figure_{index}.png'))\n",
        "\n",
        "    save_csv(train_rewards, 'train_rewards.csv')\n",
        "\n",
        "\n",
        "def evaluate_dqn(env, agent, num_episodes):\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        total_reward = 0\n",
        "        while True:\n",
        "            # 모델을 이용해 액션 결정 (탐험은 하지 않음)\n",
        "            action = agent.select_action(state, 1)\n",
        "\n",
        "            observation, reward, done, _ = env.step(action.item())\n",
        "            total_reward = min(200, total_reward + reward)\n",
        "\n",
        "            # print(env.action_space[action.item()])\n",
        "            # env.render()\n",
        "            # print(reward)\n",
        "            # print(total_reward)\n",
        "            # env.render()\n",
        "            # input()\n",
        "\n",
        "            if done:\n",
        "                next_state = None\n",
        "            else:\n",
        "                next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "            if done:\n",
        "                eval_rewards.append(total_reward)\n",
        "                break\n",
        "\n",
        "    print('Complete')\n",
        "\n",
        "    save_csv(eval_rewards, 'eval_rewards.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez7S3aVqKiCZ"
      },
      "source": [
        "#MAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkGODAPUqiKT",
        "outputId": "93662619-687c-4039-df5a-b936040671c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yes\n"
          ]
        }
      ],
      "source": [
        "env = TileGameEnv(layout=level0, max_card_usage=13, max_swap=12)  # TileGameEnv 인스턴스화\n",
        "\n",
        "# 환경 및 에이전트 초기화\n",
        "n_observations = len(env.reset())  # 환경으로부터 얻은 상태 크기\n",
        "n_actions = len(env.action_space)  # 사용할 수 있는 카드 행동 유형의 수\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "agent = DQNAgent(n_observations, n_actions, BATCH_SIZE)\n",
        "file_name = 'model_checkpoint.pth'  # 검사할 파일 이름\n",
        "\n",
        "if os.path.isfile(file_name):\n",
        "    print(\"yes\")\n",
        "    checkpoint = torch.load('model_checkpoint.pth')\n",
        "    agent.policy_net.load_state_dict(checkpoint['policy_model_state_dict'])\n",
        "    agent.target_net.load_state_dict(checkpoint['target_model_state_dict'])\n",
        "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# DQN 학습 시작\n",
        "cnt = 0\n",
        "while cnt < 20:\n",
        "  train_dqn(env, agent, 1000)\n",
        "  evaluate_dqn(env, agent, 100)\n",
        "  cnt += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4Z7_z9FRCXj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 파라미터 수 계산\n",
        "params_layer1 = (n_observations + 1) * 512\n",
        "params_layer2 = (512 + 1) * 512\n",
        "params_layer3 = (512 + 1) * 512\n",
        "params_layer4 = (512 + 1) * 512\n",
        "params_layer5 = (512 + 1) * 512\n",
        "params_layer6 = (512 + 1) * n_actions\n",
        "\n",
        "# 전체 모델 파라미터 수\n",
        "total_params = params_layer1 + params_layer2 + params_layer3 + params_layer4 + params_layer5 + params_layer6\n",
        "\n",
        "# 모델 크기 (가정: 각 파라미터가 4바이트인 경우)\n",
        "model_size_bytes = total_params * 4 / (1024**2)  # MB 단위로 변환\n",
        "\n",
        "print(f\"모델의 크기: 약 {model_size_bytes:.2f} MB\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/E7Pneq4dYSEqwKjX8QdR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}