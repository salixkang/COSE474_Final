{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZzH5aFU_q70w"
      ],
      "authorship_tag": "ABX9TyP8fUi5Y+96WqCA/baEe1LX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salixkang/COSE474_Final/blob/main/Final_Project_2018320223_%EA%B0%95%EC%8A%B9%EB%AA%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Project 2018320223 컴퓨터학과 강승모"
      ],
      "metadata": {
        "id": "keDMok7Ur2sr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Un3BqHiReT3W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from itertools import count\n",
        "from collections import namedtuple, deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "# if GPU is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TileGameEnv"
      ],
      "metadata": {
        "id": "ZzH5aFU_q70w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TileGameEnv:\n",
        "    def __init__(self, layout, max_card_usage, max_swap):\n",
        "        self.initial_layout = np.array(layout)\n",
        "        self.layout = None\n",
        "        self.max_card_usage = max_card_usage\n",
        "        self.total_normal_card_types = 10\n",
        "        self.max_enhancement = 2\n",
        "        self.max_swap = max_swap\n",
        "        self.action_space = [0, 1, 2, 3]  # 0: Use Card 1, 1: Use Card 2, 2: Swap Card 1, 3: Swap Card 2\n",
        "        self.tile_types = ['empty', '정상', '왜곡', '재배치', '축복', '추가', '강화', '복제', '신비']\n",
        "        self.special_tile_effects = {\n",
        "            '재배치': self._effect_rearrange,\n",
        "            '축복': self._effect_blessing,\n",
        "            '추가': self._effect_extra,\n",
        "            '강화': self._effect_enhance,\n",
        "            '복제': self._effect_clone,\n",
        "            '신비': self._effect_mystery\n",
        "        }\n",
        "        self.card_types = ['충격파', '낙뢰', '용오름', '해일', '지진', '폭풍우', '대폭발', '정화', '벼락', '업화', '분출', '세계수의 공명']\n",
        "        self.card_effects = {\n",
        "            '충격파': self._effect_shockwave,\n",
        "            '낙뢰': self._effect_thunder,\n",
        "            '용오름': self._effect_dragon_rise,\n",
        "            '해일': self._effect_tidal_wave,\n",
        "            '지진': self._effect_earthquake,\n",
        "            '폭풍우': self._effect_storm,\n",
        "            '대폭발': self._effect_explosion,\n",
        "            '정화': self._effect_purification,\n",
        "            '벼락': self._effect_lightning,\n",
        "            '업화': self._effect_upfire,\n",
        "            '분출': self._effect_eruption,\n",
        "            '세계수의 공명': self._effect_world_tree_resonance\n",
        "        }\n",
        "        self.current_special_tile = None  # 현재 스텝에서 생성된 특수 타일의 위치\n",
        "        self.hand = []\n",
        "        self.reserve = []\n",
        "        self.hand_enhancements = [0, 0]\n",
        "        self.cards_used = 0\n",
        "        self.swap_count = 0\n",
        "        self.used_card_index = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # 게임 초기화\n",
        "        self.layout = np.copy(self.initial_layout)\n",
        "        self.hand = [random.randint(0, self.total_normal_card_types - 1) for _ in range(2)]  # 두 장의 패\n",
        "        self.reserve = [random.randint(0, self.total_normal_card_types - 1) for _ in range(3)]  # 세 장의 다음 카드\n",
        "        self.hand_enhancements = [0, 0]  # 패에 있는 각 카드의 강화 상태\n",
        "        self.cards_used = 0\n",
        "        self.swap_count = 0\n",
        "        self.used_card_index = None\n",
        "        self.current_special_tile = None\n",
        "        self._check_enhancement(0)\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action, position = (0, 0)):\n",
        "        assert action not in self.action_space, \"Invalid action!\"\n",
        "        reward = 0\n",
        "\n",
        "        # Action: Use Card 1\n",
        "        if action == 0:\n",
        "            self.used_card_index = 0\n",
        "            self._play_card(0, position)\n",
        "            if self.current_special_tile:\n",
        "                self.layout[self.current_special_tile] = 1\n",
        "                self._generate_special_tile()\n",
        "            else:\n",
        "                self._generate_special_tile()\n",
        "\n",
        "        # Action: Use Card 2\n",
        "        elif action == 1:\n",
        "            self.used_card_index = 1\n",
        "            self._play_card(1, position)\n",
        "            if self.current_special_tile:\n",
        "                self.layout[self.current_special_tile] = 1\n",
        "                self._generate_special_tile()\n",
        "            else:\n",
        "                self._generate_special_tile()\n",
        "\n",
        "        # Action: Swap Card 1\n",
        "        elif action == 2:\n",
        "            self._swap_card(0)\n",
        "\n",
        "        # Action: Swap Card 2\n",
        "        elif action == 3:\n",
        "            self._swap_card(1)\n",
        "\n",
        "        done, result = self._is_game_over()\n",
        "        if result == \"Win\":\n",
        "            reward = 1\n",
        "        elif result == \"Lose\":\n",
        "            reward = -1\n",
        "\n",
        "        return self._get_state(), reward, done, {}\n",
        "\n",
        "    def _get_state(self):\n",
        "        return np.array(self.layout.flatten().tolist() + self.hand + self.reserve + self.hand_enhancements)\n",
        "\n",
        "    def _get_valid_positions(self, card_index):\n",
        "        # 카드의 종류에 따라 왜곡된 타일에도 사용할 수 있는지 여부를 확인\n",
        "        can_use_on_distorted = self.card_types[self.hand[card_index]] in ['정화', '세계수의 공명']\n",
        "\n",
        "        # 가능한 위치들을 찾아 리스트로 반환\n",
        "        valid_positions = []\n",
        "        for x in range(self.layout.shape[0]):\n",
        "            for y in range(self.layout.shape[1]):\n",
        "                if self.layout[x, y] == 1 or (3 <= self.layout[x, y] <= 9):\n",
        "                    valid_positions.append((x, y))\n",
        "                elif can_use_on_distorted and self.layout[x, y] == 2:\n",
        "                    valid_positions.append((x, y))\n",
        "        return valid_positions\n",
        "\n",
        "    # 선택된 카드를 사용하는 메소드\n",
        "    def _play_card(self, card_index, position):\n",
        "        card_type = self.card_types[self.hand[card_index]]\n",
        "\n",
        "        card_effect = self.card_effects.get(card_type, None)\n",
        "\n",
        "        if card_effect:\n",
        "            card_effect(position, self.hand_enhancements[card_index])\n",
        "\n",
        "        self.hand[card_index] = self.reserve.pop(0)  # 예비 카드로 교체\n",
        "        self.hand_enhancements[card_index] = 0  # 사용된 카드 강화 상태 리셋\n",
        "\n",
        "        self.reserve.append(self._draw_new_card())  # 예비 카드 보충\n",
        "        self.cards_used += 1 # 카드 사용 횟수 증가\n",
        "\n",
        "        # 카드가 강화 가능한지 체크\n",
        "        self._check_enhancement(card_index)\n",
        "\n",
        "    # 카드를 교체하는 메소드\n",
        "    def _swap_card(self, card_index):\n",
        "        if self.swap_count < self.max_swap:\n",
        "            self.swap_count += 1  # 교체 횟수 증가\n",
        "            self.hand[card_index] = self.reserve.pop(0)  # 예비 카드로 교체\n",
        "            self.hand_enhancements[card_index] = 0  # 사용된 카드 강화 상태 리셋\n",
        "            self.reserve.append(self._draw_new_card())  # 예비 카드 보충\n",
        "            self._check_enhancement(card_index)  # 강화 로직 적용\n",
        "\n",
        "    # 새로운 카드를 예비 카드에 추가하는 메소드\n",
        "    def _draw_new_card(self):\n",
        "        return random.randint(0, self.total_normal_card_types - 1)\n",
        "\n",
        "    def _check_enhancement(self, new_card_index):\n",
        "        other_card_index = 1 - new_card_index\n",
        "        if self.hand[new_card_index] == self.hand[other_card_index]:\n",
        "            if self.hand_enhancements[other_card_index] < self.max_enhancement:\n",
        "                self.hand_enhancements[other_card_index] += 1  # 카드 강화\n",
        "                self.hand[new_card_index] = self.reserve.pop(0)  # 다음 예비 카드를 패로 이동\n",
        "                self.hand_enhancements[new_card_index] = 0\n",
        "                self.reserve.append(self._draw_new_card())  # 예비 카드 보충\n",
        "                self._check_enhancement(new_card_index)\n",
        "\n",
        "    # 타일 파괴\n",
        "    def _destroy_tile(self, position, enhancement=0, special=0, ):\n",
        "        x, y = position\n",
        "        tile_type = self.tile_types[self.layout[x, y]]\n",
        "\n",
        "        if tile_type == '정상':\n",
        "            self.layout[x, y] = 0\n",
        "        elif tile_type == '왜곡':\n",
        "            self._destroy_distorted_tile(position, enhancement, special)\n",
        "        elif tile_type == 'empty':\n",
        "            self.layout[x, y] = 0\n",
        "        else:\n",
        "            special_tile_effect = self.special_tile_effects.get(tile_type, None)\n",
        "            self.layout[x, y] = 0\n",
        "            self.current_special_tile = None\n",
        "            if special_tile_effect:\n",
        "                special_tile_effect()\n",
        "\n",
        "    # 특정 위치가 게임판 내에 있는지 확인하는 메소드\n",
        "    def _is_within_bounds(self, position):\n",
        "        x, y = position\n",
        "        return 0 <= x < self.layout.shape[0] and 0 <= y < self.layout.shape[1]\n",
        "\n",
        "    # 랜덤 위치에 정상 타일을 생성하는 메소드\n",
        "    def _create_random_tiles(self, count):\n",
        "        for _ in range(count):\n",
        "            empty_positions = [(i, j) for i in range(self.layout.shape[0])\n",
        "                               for j in range(self.layout.shape[1]) if self.layout[i][j] == 0]\n",
        "            if empty_positions:\n",
        "                x, y = random.choice(empty_positions)\n",
        "                self.layout[x, y] = 1  # 정상 타일 생성\n",
        "\n",
        "    # 왜곡된 타일을 파괴했을 때 정상 타일을 생성하는 메서드\n",
        "    def _destroy_distorted_tile(self, position, enhancement, special = 0):\n",
        "        # 파괴할 위치에 왜곡된 타일이 있는지 확인\n",
        "        if self.layout[position] == 2:\n",
        "            self.layout[position] = 0  # 왜곡된 타일 파괴\n",
        "\n",
        "            if enhancement != 2 or special == 1: # 강화 단계가 2거나 특수한 상황이라면 페널티 발생하지 않음\n",
        "                # 타일이 없는 위치 중 무작위로 세 개의 위치에 정상 타일 생성\n",
        "                empty_positions = np.argwhere(self.layout == 0)\n",
        "                if empty_positions.size > 0:\n",
        "                    for _ in range(min(3, len(empty_positions))):  # 최대 3개 혹은 가능한 개수만큼\n",
        "                        x, y = empty_positions[random.randint(0, len(empty_positions) - 1)]\n",
        "                        self.layout[x, y] = 1  # 정상 타일 생성\n",
        "\n",
        "    # 남은 정상 타일 중 하나에 무작위 특수 타일 생성 메소드\n",
        "    def _generate_special_tile(self):\n",
        "        # 이전 특수 타일 위치 초기화\n",
        "        self.current_special_tile = None\n",
        "        normal_tiles = list(zip(*np.where(self.layout == 1)))\n",
        "        if normal_tiles:  # 정상 타일이 존재하는 경우\n",
        "            x, y = random.choice(normal_tiles)\n",
        "            special_tile_type = random.randint(3, len(self.tile_types) - 1)\n",
        "            self.layout[x, y] = special_tile_type\n",
        "            self.current_special_tile = (x, y)  # 생성된 특수 타일의 위치 저장\n",
        "\n",
        "    # 충격파\n",
        "    def _effect_shockwave(self, position, enhancement):\n",
        "        # 중심 위치의 타일은 100% 확률로 파괴\n",
        "        center_x, center_y = position\n",
        "        self._destroy_tile((center_x, center_y), enhancement)\n",
        "\n",
        "        # 3x3 영역의 타일을 파괴하는 로직\n",
        "        for i in range(center_x - 1, center_x + 2):\n",
        "            for j in range(center_y - 1, center_y + 2):\n",
        "                # 배열의 범위를 벗어나지 않는지 체크\n",
        "                if self._is_within_bounds((i, j)):\n",
        "                    # 중심 타일이 아닌 경우에만 확률 체크\n",
        "                    if (i, j) != position:\n",
        "                        if random.random() < 0.75 or enhancement > 0: # 75% or 강화되면 파괴\n",
        "                            self._destroy_tile((i, j))\n",
        "\n",
        "    # 낙뢰\n",
        "    def _effect_thunder(self, position, enhancement):\n",
        "        # 중심 위치는 항상 파괴\n",
        "        center_x, center_y = position\n",
        "        self._destroy_tile((center_x, center_y), enhancement)\n",
        "\n",
        "        # 십자가 형태로 영향을 주므로 상하좌우 타일 파괴\n",
        "        offsets = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "        for dx, dy in offsets:\n",
        "            x, y = center_x + dx, center_y + dy\n",
        "            if self._is_within_bounds((x, y)):\n",
        "                if random.random() < 0.5 or enhancement > 0:  # 50% 확률 or 강화되면 파괴\n",
        "                    self._destroy_tile((x, y), enhancement)\n",
        "\n",
        "    # 용오름\n",
        "    def _effect_dragon_rise(self, position, enhancement):\n",
        "        # 중심 위치는 항상 파괴\n",
        "        center_x, center_y = position\n",
        "        self._destroy_tile((center_x, center_y), enhancement)\n",
        "\n",
        "        # X 자 형태로 영향을 주므로 대각선 타일 파괴\n",
        "        diagonals = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "        for dx, dy in diagonals:\n",
        "            x, y = center_x + dx, center_y + dy\n",
        "            if self._is_within_bounds((x, y)):\n",
        "                if random.random() < 0.5 or enhancement > 0:  # 50% 확률로 or 강화되면 파괴\n",
        "                    self._destroy_tile((x, y), enhancement)\n",
        "\n",
        "    # 해일\n",
        "    def _effect_tidal_wave(self, position, enhancement):\n",
        "        # 중심 위치는 100% 확률로 파괴\n",
        "        center_x, center_y = position\n",
        "        self._destroy_tile((center_x, center_y), enhancement)\n",
        "\n",
        "        # 파괴 확률을 위한 기본값 설정\n",
        "        base_prob = 1.0\n",
        "        decrease_step = 0.15\n",
        "\n",
        "        # 십자가 형태로 타일을 파괴\n",
        "        # 상하 방향\n",
        "        for i in range(self.layout.shape[0]):\n",
        "            if i != center_x:  # 중심 위치 제외\n",
        "                destroy_prob = base_prob - decrease_step * abs(i - center_x)\n",
        "                if random.random() < destroy_prob or enhancement > 0:\n",
        "                    self._destroy_tile((i, center_y), enhancement)\n",
        "\n",
        "        # 좌우 방향\n",
        "        for j in range(self.layout.shape[1]):\n",
        "            if j != center_y:  # 중심 위치 제외\n",
        "                destroy_prob = base_prob - decrease_step * abs(j - center_y)\n",
        "                if random.random() < destroy_prob or enhancement > 0:\n",
        "                    self._destroy_tile((center_x, j), enhancement)\n",
        "\n",
        "    # 지진\n",
        "    def _effect_earthquake(self, position, enhancement):\n",
        "        # '지진' 카드: 시전 위치를 중심으로 가로 일렬 범위 모든 타일 타격\n",
        "        for j in range(self.layout.shape[1]):\n",
        "            destroy_prob = 1.0 - 0.15 * abs(j - position[1])\n",
        "            if random.random() < destroy_prob or enhancement > 0:\n",
        "                self._destroy_tile((position[0], j), enhancement)\n",
        "\n",
        "    # 폭풍우\n",
        "    def _effect_storm(self, position, enhancement):\n",
        "        # '폭풍우' 카드: 시전 위치를 중심으로 세로 일렬 범위 모든 타일 타격\n",
        "        for i in range(self.layout.shape[0]):\n",
        "            destroy_prob = 1.0 - 0.15 * abs(i - position[0])\n",
        "            if random.random() < destroy_prob or enhancement > 0:\n",
        "                self._destroy_tile((i, position[1]), enhancement)\n",
        "\n",
        "    # 대폭발\n",
        "    def _effect_explosion(self, position, enhancement):\n",
        "        # '대폭발' 카드: 시전 위치 중심으로 X 자 형태 모든 타일 타격\n",
        "        for offset in range(-self.layout.shape[0], self.layout.shape[0]):\n",
        "            destroy_prob = 1.0 - 0.15 * abs(offset)\n",
        "            if random.random() < destroy_prob or enhancement > 0:\n",
        "                if 0 <= position[0] + offset < self.layout.shape[0]:\n",
        "                    self._destroy_tile((position[0] + offset, position[1] + offset), enhancement)\n",
        "                    self._destroy_tile((position[0] + offset, position[1] - offset), enhancement)\n",
        "\n",
        "    # 정화\n",
        "    def _effect_purification(self, position, enhancement):\n",
        "        # '정화' 카드: 시전 위치 중심으로 좌우 타일 타격\n",
        "        special = 1\n",
        "        center_x, center_y = position\n",
        "        self._destroy_tile((center_x, center_y), enhancement, special)  # 중심 타일은 항상 파괴\n",
        "        for offset in [-1, 1]:\n",
        "            if 0 <= position[1] + offset < self.layout.shape[1]:\n",
        "                if random.random() < 0.5 or enhancement > 0:  # 좌우 타일은 50% 확률로 or 강화되면 파괴\n",
        "                    self._destroy_tile((position[0], position[1] + offset), enhancement, special)\n",
        "            if enhancement == 2:\n",
        "                if 0 <= position[0] + offset < self.layout.shape[0]:\n",
        "                    self._destroy_tile((position[0] + offset, position[1]), enhancement, special)\n",
        "\n",
        "    # 벼락\n",
        "    def _effect_lightning(self, position, enhancement):\n",
        "        # '벼락' 카드: 시전 위치 100% 파괴, 랜덤으로 0~2칸 추가 파괴\n",
        "        self._destroy_tile(position)\n",
        "\n",
        "        all_tiles = [(i, j) for i in range(self.layout.shape[0])\n",
        "                     for j in range(self.layout.shape[1])\n",
        "                     if self.layout[i, j] != 2 and self.layout[i, j] != 0]\n",
        "\n",
        "        num_tiles_to_destroy = random.randint(0, min(2 + 2 * enhancement, len(all_tiles)))\n",
        "\n",
        "        for _ in range(num_tiles_to_destroy):\n",
        "            tile_to_destroy = random.choice(all_tiles)\n",
        "            self._destroy_tile(tile_to_destroy)\n",
        "            all_tiles.remove(tile_to_destroy)  # 파괴된 타일은 리스트에서 제거\n",
        "\n",
        "        # 랜덤 위치에 랜덤으로 0~1칸 정상 타일 생성\n",
        "        self._create_random_tiles(random.randint(0, 1))\n",
        "\n",
        "    # 업화\n",
        "    def _effect_upfire(self, position, enhancement):\n",
        "        # '업화' 카드: 시전 위치 중심으로 마름모 형태 2칸 범위 내 파괴, 시전 위치 100% 파괴\n",
        "        self._destroy_tile(position, enhancement)\n",
        "        offsets = [(-2, 0), (-1, -1), (-1, 0), (-1, 1), (0, -2), (0, -1), (0, 1), (0, 2), (1, -1), (1, 0), (1, 1), (2, 0)]\n",
        "        for dx, dy in offsets:\n",
        "            new_position = (position[0] + dx, position[1] + dy)\n",
        "            if self._is_within_bounds(new_position) and random.random() < 0.5:\n",
        "                if random.random() < 0.5 or enhancement > 0:\n",
        "                    self._destroy_tile(new_position, enhancement)\n",
        "\n",
        "    # 분출\n",
        "    def _effect_eruption(self, position, enhancement = 0):\n",
        "        # '분출' 카드: 시전 위치 100% 파괴\n",
        "        self._destroy_tile(position)\n",
        "\n",
        "    # 세계수의 공명\n",
        "    def _effect_world_tree_resonance(self, position, enhancement = 0):\n",
        "        # '세계수의 공명' 카드: 시전 위치 중심으로 십자형 타일 2칸 범위 100% 파괴\n",
        "        special = 1\n",
        "        offsets = [(0, 0), (-2, 0), (-1, 0), (1, 0), (2, 0), (0, -2), (0, -1), (0, 1), (0, 2)]\n",
        "        for dx, dy in offsets:\n",
        "            new_position = (position[0] + dx, position[1] + dy)\n",
        "            if self._is_within_bounds(new_position):\n",
        "                self._destroy_tile(new_position, enhancement, special)\n",
        "\n",
        "    # 특수 타일의 효과를 적용하는 메소드들\n",
        "    def _effect_rearrange(self):\n",
        "        # '재배치' 효과: 남은 모든 타일을 무작위 위치로 재배치\n",
        "        tiles = [(x, y, self.layout[x, y]) for x in range(self.layout.shape[0])\n",
        "                 for y in range(self.layout.shape[1]) if self.layout[x, y] != -1]\n",
        "\n",
        "        # 비정상 위치를 제외한 나머지 타일 값만 추출\n",
        "        tile_values = [value for _, _, value in tiles if value != -1]\n",
        "\n",
        "        # 추출한 타일 값을 무작위로 섞음\n",
        "        random.shuffle(tile_values)\n",
        "\n",
        "        # 무작위로 섞은 타일 값을 다시 게임 보드에 배치\n",
        "        for (x, y, _), value in zip(tiles, tile_values):\n",
        "            self.layout[x, y] = value\n",
        "\n",
        "    def _effect_blessing(self):\n",
        "        # '축복' 효과: 이번 턴의 카드 사용 횟수를 증가시키지 않음\n",
        "        self.cards_used -= 1\n",
        "\n",
        "    def _effect_extra(self):\n",
        "        # '추가' 효과: 최대 카드 교체 횟수를 하나 늘림\n",
        "        self.max_swaps += 1\n",
        "\n",
        "    def _effect_enhance(self):\n",
        "        # '강화' 효과: 패에 있는 카드를 1회 강화\n",
        "        if self.hand_enhancements[1 - self.used_card_index] < 2:\n",
        "            self.hand_enhancements[1 - self.used_card_index] += 1\n",
        "\n",
        "    def _effect_clone(self):\n",
        "        # '복제' 효과: 마지막으로 사용한 카드로 교체\n",
        "        self.hand[1 - self.used_card_index] = self.hand[1 - self.used_card_index]\n",
        "        self.hand_enhancements[1 - self.used_card_index] = self.hand_enhancements[1 - self.used_card_index]\n",
        "    def _effect_mystery(self):\n",
        "        # '신비' 효과: 랜덤하게 '세계수의 공명' 또는 '분출'로 교체\n",
        "        self.hand[1 - self.used_card_index] = 11 if random.random() < 0.5 else 12\n",
        "\n",
        "    def _is_game_over(self):\n",
        "        # 정상 타일(1)과 특수 타일(3~9)이 없는지 확인\n",
        "        normal_tiles_gone = np.all((self.layout != 1) & (self.layout < 3))\n",
        "        # 카드 사용 및 교체 횟수가 규정 내인지 확인\n",
        "        within_card_usage = self.cards_used <= self.max_card_usage\n",
        "        within_swap_count = self.swap_count <= self.max_swap\n",
        "\n",
        "        if normal_tiles_gone and within_card_usage and within_swap_count:\n",
        "            return True, \"Win\"\n",
        "        elif not within_card_usage or not within_swap_count:\n",
        "            return True, \"Lose\"\n",
        "        else:\n",
        "            return False, \"\"\n",
        "\n",
        "    def render(self):\n",
        "        # 환경 상태 출력\n",
        "        print(\"Layout:\")\n",
        "        print(self.layout)\n",
        "        print(\"Hand: \", [self.card_types[self.hand[0]], self.card_types[self.hand[1]]])\n",
        "        print(\"Reserve: \", [self.card_types[self.reserve[0]],self.card_types[self.reserve[1]], self.card_types[self.reserve[2]]])\n",
        "        print(\"Cards used: \", self.cards_used)\n",
        "        print(\"Swaps used: \", self.swap_count)\n",
        "        print(\"Enhancements: \", self.hand_enhancements)\n",
        "\n",
        "\n",
        "level1 = [[0, 0, 1, 1, 0, 0], [0, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 0], [0, 0, 1, 1, 0, 0]]\n",
        "level2 = [[0, 0, 1, 1, 0, 0], [0, 1, 1, 1, 1, 0], [1, 1, 2, 2, 1, 1], [1, 1, 2, 2, 1, 1], [0, 1, 1, 1, 1, 0], [0, 0, 1, 1, 0, 0]]\n",
        "\n"
      ],
      "metadata": {
        "id": "v5__vczsqQkE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DQN"
      ],
      "metadata": {
        "id": "YWhjdHN2rDZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 아키텍처 정의\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "mV1AYuzWInIH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN 에이전트 정의\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.gamma = 0.99  # Discount factor\n",
        "        self.epsilon = 1.0  # Exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.model = DQN(state_size, action_size).to(device)\n",
        "        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001, amsgrad=True)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() <= self.epsilon:\n",
        "            # Explore: 무작위 행동을 취함\n",
        "            action_type = random.choice([0, 1, 2, 3])  # 카드 사용 또는 교체 중 선택\n",
        "            if action_type in [0, 1]:  # 카드 사용 행동인 경우\n",
        "                valid_positions = env._get_valid_positions(action_type)  # 현재 유효한 위치 가져오기\n",
        "                position = random.choice(valid_positions)  # 무작위 유효한 위치 선택\n",
        "            else:  # 카드 교체 행동인 경우\n",
        "                position = None\n",
        "        else:\n",
        "            # Exploit: 신경망을 사용하여 행동을 선택함\n",
        "            state = torch.FloatTensor(state).unsqueeze(0)\n",
        "            q_values = self.model(state)\n",
        "            action_type = torch.argmax(q_values).item() % 4  # 행동 유형 추출\n",
        "            if action_type in [0, 1]:  # 카드 사용 행동인 경우\n",
        "                valid_positions = env._get_valid_positions()  # 현재 유효한 위치 가져오기\n",
        "                position = torch.argmax(q_values).item() // 4  # 위치 인덱스 추출\n",
        "                position = valid_positions[position]  # 실제 유효한 위치로 변환\n",
        "            else:  # 카드 교체 행동인 경우\n",
        "                position = None\n",
        "\n",
        "        return (action_type, position)\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            action_type, action_position = action\n",
        "            state = torch.FloatTensor(state)\n",
        "            next_state = torch.FloatTensor(next_state)\n",
        "            reward = torch.FloatTensor([reward])\n",
        "            done = torch.FloatTensor([done])\n",
        "            # action_type이 카드 사용인 경우 유효한 위치 인덱스를 가져와서 조합\n",
        "            if action_type in [0, 1]:\n",
        "                # 행동 유형(action_type)과 행동 위치(action_position)를 이용해 전체 행동 인덱스 계산\n",
        "                action_idx = torch.LongTensor([[action_type + action_position]])\n",
        "            else:\n",
        "                # 카드 교체인 경우 행동 유형(action_type)만 사용\n",
        "                action_idx = torch.LongTensor([[action_type]])# 현재 상태의 Q 값\n",
        "\n",
        "            current_q_values = self.model(state).gather(1, action_idx)\n",
        "\n",
        "            # 다음 상태에서 예측된 최대 Q 값\n",
        "            max_next_q_values = self.model(next_state).detach().max(1)[0]\n",
        "\n",
        "            # 종료 플래그가 True인 경우 다음 상태의 Q 값은 0이 됨\n",
        "            # Q-Learning 타겟 계산\n",
        "            target_q_values = reward + (self.gamma * max_next_q_values * (1 - done))\n",
        "\n",
        "             # 손실 계산 및 역전파\n",
        "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "xWaz6A4lmM7U"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 루프 정의\n",
        "def train_dqn(env, agent, num_episodes, batch_size):\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            print(action)\n",
        "            next_state, reward, done, _ = env.step(action)  # 환경에 행동 적용\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                print(f\"Episode: {episode+1}, Total reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
        "                break\n",
        "            if len(agent.memory) > batch_size:\n",
        "                agent.replay(batch_size)\n",
        "        # Epsilon 감소\n",
        "        agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)"
      ],
      "metadata": {
        "id": "w66D1Ph4pew0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = TileGameEnv(layout=level1, max_card_usage=5, max_swap=2)  # TileGameEnv 인스턴스화\n",
        "\n",
        "# 환경 및 에이전트 초기화\n",
        "state_size = len(env.reset())  # 환경으로부터 얻은 상태 크기\n",
        "action_size = 4  # 사용할 수 있는 카드 행동 유형의 수\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "num_episodes = 5  # 학습할 에피소드의 수\n",
        "batch_size = 32\n",
        "# DQN 학습 시작\n",
        "train_dqn(env, agent, num_episodes, batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "id": "ZkGODAPUqiKT",
        "outputId": "5608b6b8-4e2b-4fec-835e-f6655402eac1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, (0, 2))\n",
            "(1, (0, 3))\n",
            "(3, None)\n",
            "(0, (5, 2))\n",
            "(3, None)\n",
            "(3, None)\n",
            "(0, (1, 3))\n",
            "(1, (1, 3))\n",
            "(3, None)\n",
            "(2, None)\n",
            "(1, (5, 2))\n",
            "(0, (1, 2))\n",
            "(0, (1, 4))\n",
            "(0, (4, 3))\n",
            "(0, (4, 2))\n",
            "(1, (5, 2))\n",
            "(0, (3, 0))\n",
            "(0, (2, 2))\n",
            "(1, (1, 3))\n",
            "(3, None)\n",
            "(3, None)\n",
            "(1, (4, 4))\n",
            "(3, None)\n",
            "(1, (3, 1))\n",
            "(1, (3, 0))\n",
            "(2, None)\n",
            "(2, None)\n",
            "(3, None)\n",
            "(2, None)\n",
            "(1, (3, 1))\n",
            "(3, None)\n",
            "(3, None)\n",
            "(0, (5, 3))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-8041af320bfb>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# DQN 학습 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-eb32dc035533>\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m(env, agent, num_episodes, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Epsilon 감소\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-7f36422f6de0>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0maction_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# 현재 상태의 Q 값\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mcurrent_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# 다음 상태에서 예측된 최대 Q 값\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
        "# GAMMA is the discount factor as mentioned in the previous section\n",
        "# EPS_START is the starting value of epsilon\n",
        "# EPS_END is the final value of epsilon\n",
        "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
        "# TAU is the update rate of the target network\n",
        "# LR is the learning rate of the ``AdamW`` optimizer\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "\n",
        "env = TileGameEnv(layout=level1, max_card_usage=5, max_swap=2)\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space\n",
        "# Get the number of state observations\n",
        "state = env.reset()\n",
        "n_observations = len(state)\n",
        "\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "steps_done = 0\n"
      ],
      "metadata": {
        "id": "oHr9Gk3wP2o9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return the largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.choice(env.action_space)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations(show_result=False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())"
      ],
      "metadata": {
        "id": "szViF6SGUwuR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "MrXNfzCeWa2Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    num_episodes = 600\n",
        "else:\n",
        "    num_episodes = 50\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and get it's state\n",
        "    state = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    for t in count():\n",
        "        action = select_action(state)\n",
        "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if terminated:\n",
        "            next_state = None\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        # θ′ ← τ θ + (1 −τ )θ′\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "\n",
        "print('Complete')\n",
        "plot_durations(show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "a828XrqqWd5m",
        "outputId": "2d68eb0b-a987-4575-a402-066cdd115864"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e6d015feeb38>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TileGameEnv.step() missing 1 required positional argument: 'position'"
          ]
        }
      ]
    }
  ]
}